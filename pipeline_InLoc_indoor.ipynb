{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be77fcfa",
   "metadata": {},
   "source": [
    "# From Images to a Place Memory: VPR Tutorial\n",
    "\n",
    "This notebook demonstrates **Visual Place Recognition (VPR)** for robot localization using **NetVLAD** and a **Vector Database**.\n",
    "\n",
    "## Overview\n",
    "\n",
    "**SLAM provides precise motion tracking. VPR provides long-term place recognition.**  \n",
    "Combined, they enable reliable indoor localization over time.\n",
    "\n",
    "### What You'll Learn\n",
    "\n",
    "**Part 1 — Building Place Memory (Offline)**\n",
    "- Step 1: Turn images into \"Place Signatures\" using NetVLAD\n",
    "- Step 2: Store these signatures into a Vector Database with pose metadata\n",
    "\n",
    "**Part 2 — Querying Place Memory (Online)**\n",
    "- Step 1: Convert current camera view → query vector\n",
    "- Step 2: KNN search in Vector DB → retrieve similar places\n",
    "- Step 3: Map candidates to coarse locations\n",
    "\n",
    "---\n",
    "\n",
    "## Setup & Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0094bfb2-5a3d-47b4-b10d-bb3ad8f8c5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sys\n",
    "import sys\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "# math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "# IO\n",
    "import h5py\n",
    "import yaml\n",
    "\n",
    "# utils\n",
    "from termcolor import colored\n",
    "from pprint import pformat\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aace6f9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU ON:  True\n",
      "Using local vpr_core from: /home/gx-shu/My/RevisitVLOC\n"
     ]
    }
   ],
   "source": [
    "vpr_root = Path(\".\")  # Use local vpr_core copy\n",
    "sys.path.insert(0, str(vpr_root))\n",
    "\n",
    "from vpr_core import (extract_features,\n",
    "                  pairs_from_retrieval,\n",
    "                  pairs_from_retrieval)\n",
    "\n",
    "\n",
    "print(\"GPU ON: \", torch.cuda.is_available())\n",
    "print(\"Using local vpr_core from:\", vpr_root.resolve())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206c7a31",
   "metadata": {},
   "outputs": [],
   "source": "# Data paths\ndata_dir = Path(\"./data/netvlad\")\nimages_train = data_dir / \"train_mono\"  # Reference images with vertex metadata\nimages_test = data_dir / \"test\"          # Query images\noutputs = data_dir / \"outputs\"\noutputs.mkdir(exist_ok=True)\n\nprint(f\"Train images: {images_train} ({len(list(images_train.glob('*.png')))} files)\")\nprint(f\"Test images:  {images_test} ({len(list(images_test.glob('*.png')))} files)\")\nprint(f\"Outputs:      {outputs}\")"
  },
  {
   "cell_type": "markdown",
   "id": "4aab905b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 1 — Building Place Memory (Offline)\n",
    "\n",
    "When the robot explores, we build a **place memory** by:\n",
    "1. Converting each image into a compact **global descriptor** (place signature)\n",
    "2. Storing these descriptors in a **vector database** with pose metadata from SLAM\n",
    "\n",
    "This creates a searchable map of visual memories."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c742622",
   "metadata": {},
   "source": [
    "## Step 1 — Image → Place Signature (NetVLAD)\n",
    "\n",
    "When a robot captures a frame, **NetVLAD** transforms it into a single vector using two key components:\n",
    "\n",
    "### 1) CNN Backbone: Learning Visual Patterns\n",
    "A convolutional neural network (e.g., VGG16) extracts **feature maps** — rich patterns related to:\n",
    "- Shapes, Textures, Spatial layout\n",
    "\n",
    "These features capture what the scene *looks and feels like*, beyond just raw pixels.\n",
    "\n",
    "### 2) VLAD Layer: Compressing Features into a Vector\n",
    "**VLAD** (Vector of Locally Aggregated Descriptors) clusters local features into visual concepts and measures:\n",
    "- Which concepts are present\n",
    "- How strongly they appear\n",
    "- How they differ from learned \"centers\"\n",
    "\n",
    "**Result:** A full image becomes a compact **4096-D vector** — a global descriptor representing the identity of a place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xfmxajxok2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset GPU memory stats to track NetVLAD usage from baseline\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    print(\"GPU memory stats reset. Ready to track NetVLAD memory usage.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c86b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Configs for feature extractors:\\n{pformat(extract_features.confs)}\")\n",
    "\n",
    "feature_conf = extract_features.confs[\"netvlad\"]\n",
    "\n",
    "# Extract features for TEST set\n",
    "print(\"\\n=== Extracting NetVLAD features for TEST set ===\")\n",
    "feature_path_test = outputs / \"global-feats-netvlad-test.h5\"\n",
    "extract_features.main(feature_conf, images_test, outputs, feature_path=feature_path_test)\n",
    "print(f\"Test features saved to: {feature_path_test}\")\n",
    "\n",
    "# Extract features for TRAIN set\n",
    "print(\"\\n=== Extracting NetVLAD features for TRAIN set ===\")\n",
    "feature_path_train = outputs / \"global-feats-netvlad-train.h5\"\n",
    "extract_features.main(feature_conf, images_train, outputs, feature_path=feature_path_train)\n",
    "print(f\"Train features saved to: {feature_path_train}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fpx3a0nzip",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU memory usage\n",
    "if torch.cuda.is_available():\n",
    "    allocated = torch.cuda.memory_allocated() / 1024**3  # Convert to GB\n",
    "    reserved = torch.cuda.memory_reserved() / 1024**3\n",
    "    max_allocated = torch.cuda.max_memory_allocated() / 1024**3\n",
    "    \n",
    "    print(f\"\\n=== GPU Memory Usage ===\")\n",
    "    print(f\"Currently allocated: {allocated:.2f} GB\")\n",
    "    print(f\"Currently reserved:  {reserved:.2f} GB\")\n",
    "    print(f\"Peak allocated:      {max_allocated:.2f} GB\")\n",
    "    \n",
    "    # Get total GPU memory\n",
    "    total_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "    print(f\"Total GPU memory:    {total_memory:.2f} GB\")\n",
    "    print(f\"Memory utilization:  {(allocated/total_memory)*100:.1f}%\")\n",
    "else:\n",
    "    print(\"CUDA not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2c35e3",
   "metadata": {},
   "source": [
    "## Step 2 — Store Place Memories into Vector Database\n",
    "\n",
    "Each NetVLAD vector becomes a **memory of a place**. To use it for localization, we also store **where the robot was**:\n",
    "- Robot's pose (from SLAM system)\n",
    "- Metadata: timestamp, image path, vertex ID\n",
    "\n",
    "### Why a Vector Database?\n",
    "\n",
    "A vector database provides:\n",
    "- **Efficient long-term memory** for large environments\n",
    "- **Real-time retrieval** even with millions of vectors\n",
    "- **High-performance indexes** (IVF, HNSW) for fast similarity search\n",
    "\n",
    "**In short:** A vector DB allows robots to remember more places, find memories faster, and use them intelligently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268988c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create image pairs using NetVLAD retrieval\n",
    "top = 5  # Number of top matches per test image\n",
    "\n",
    "# Define paths for feature descriptors\n",
    "descriptors_test = feature_path_test\n",
    "descriptors_train = feature_path_train\n",
    "\n",
    "# Output pairs file\n",
    "output_pairs_file = outputs / f\"pairs-netvlad-top{top}.txt\"\n",
    "\n",
    "print(f\"\\n=== Creating image pairs (top {top} matches per test image) ===\")\n",
    "print(f\"Test descriptors:  {descriptors_test}\")\n",
    "print(f\"Train descriptors: {descriptors_train}\")\n",
    "print(f\"Output pairs: {output_pairs_file}\")\n",
    "\n",
    "pairs_from_retrieval.main(\n",
    "    descriptors=descriptors_test,\n",
    "    db_descriptors=descriptors_train,\n",
    "    output=output_pairs_file,\n",
    "    num_matched=top,\n",
    ")\n",
    "\n",
    "print(f\"\\nPairs saved to: {output_pairs_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fi0mtmmsv06",
   "metadata": {},
   "source": [
    "### Visualize Retrieval Results\n",
    "\n",
    "Let's verify the extracted features by running a simple retrieval test (without vector DB yet)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32w27mixtc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read pairs file\n",
    "pairs = []\n",
    "with open(output_pairs_file, 'r') as f:\n",
    "    for line in f:\n",
    "        test_img, train_img = line.strip().split()\n",
    "        pairs.append((test_img, train_img))\n",
    "\n",
    "# Group pairs by test image\n",
    "from collections import defaultdict\n",
    "test_matches = defaultdict(list)\n",
    "for test_img, train_img in pairs:\n",
    "    test_matches[test_img].append(train_img)\n",
    "\n",
    "print(f\"Found {len(test_matches)} test images\")\n",
    "print(f\"Each test has {len(list(test_matches.values())[0])} matches\\n\")\n",
    "\n",
    "# Create output directory for plots\n",
    "plots_dir = outputs / \"retrieval_plots\"\n",
    "plots_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Visualize ALL test images (or set a specific number)\n",
    "num_test_to_show = len(test_matches)  # Change to a number like 10 to limit output\n",
    "test_list = list(test_matches.items())[:num_test_to_show]\n",
    "\n",
    "print(f\"Generating {num_test_to_show} visualization plots...\")\n",
    "\n",
    "for idx, (test_img, matched_imgs) in enumerate(test_list):\n",
    "    num_matches = len(matched_imgs)\n",
    "    \n",
    "    fig = plt.figure(figsize=(14, 3))\n",
    "    gs = GridSpec(1, num_matches + 1, figure=fig, wspace=0.3)\n",
    "    \n",
    "    # Show test image\n",
    "    ax_test = fig.add_subplot(gs[0, 0])\n",
    "    test_path = images_test / test_img\n",
    "    img_test = plt.imread(test_path)\n",
    "    ax_test.imshow(img_test)\n",
    "    ax_test.set_title(f\"Test {idx+1}\\n{test_img}\", fontsize=10, fontweight='bold', color='red')\n",
    "    ax_test.axis('off')\n",
    "    \n",
    "    # Show top-k matched images\n",
    "    for i, match_img in enumerate(matched_imgs):\n",
    "        ax_match = fig.add_subplot(gs[0, i + 1])\n",
    "        match_path = images_train / match_img\n",
    "        img_match = plt.imread(match_path)\n",
    "        ax_match.imshow(img_match)\n",
    "        ax_match.set_title(f\"Rank {i+1}\\n{match_img}\", fontsize=9)\n",
    "        ax_match.axis('off')\n",
    "    \n",
    "    # Add title with proper spacing\n",
    "    fig.suptitle(f\"Top-{num_matches} Retrieval Results for Test Image {idx+1}\", \n",
    "                 fontsize=12, fontweight='bold')\n",
    "    fig.subplots_adjust(top=0.88)  # Adjust to prevent overlap\n",
    "    \n",
    "    # Save figure (no display in notebook)\n",
    "    save_path = plots_dir / f\"retrieval_test_{idx+1:03d}.png\"\n",
    "    plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "    plt.close(fig)  # Close to free memory\n",
    "    \n",
    "    if (idx + 1) % 10 == 0:\n",
    "        print(f\"  Progress: {idx+1}/{num_test_to_show} saved\")\n",
    "\n",
    "print(f\"\\n✓ All {num_test_to_show} plots saved to: {plots_dir}\")\n",
    "print(f\"  Example files: retrieval_test_001.png, retrieval_test_002.png, ...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2zm6oryoqii",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load feature descriptors to compute similarity scores\n",
    "print(\"Loading feature descriptors...\")\n",
    "with h5py.File(feature_path_test, 'r') as f_test:\n",
    "    test_descriptors = {name: f_test[name]['global_descriptor'][:] for name in f_test.keys()}\n",
    "\n",
    "with h5py.File(feature_path_train, 'r') as f_train:\n",
    "    train_descriptors = {name: f_train[name]['global_descriptor'][:] for name in f_train.keys()}\n",
    "\n",
    "print(f\"Loaded {len(test_descriptors)} test descriptors and {len(train_descriptors)} train descriptors\")\n",
    "\n",
    "def compute_similarity(desc1, desc2):\n",
    "    \"\"\"Compute cosine similarity between two descriptors\"\"\"\n",
    "    return np.dot(desc1, desc2) / (np.linalg.norm(desc1) * np.linalg.norm(desc2))\n",
    "\n",
    "# Read pairs file and group by test image\n",
    "pairs = []\n",
    "with open(output_pairs_file, 'r') as f:\n",
    "    for line in f:\n",
    "        test_img, train_img = line.strip().split()\n",
    "        pairs.append((test_img, train_img))\n",
    "\n",
    "# Group by test image\n",
    "from collections import defaultdict\n",
    "test_to_matches = defaultdict(list)\n",
    "for test_img, train_img in pairs:\n",
    "    test_to_matches[test_img].append(train_img)\n",
    "\n",
    "# Build DataFrame with test image, matched vertices, and similarity scores\n",
    "results = []\n",
    "\n",
    "for test_img, matched_train_imgs in test_to_matches.items():\n",
    "    row = {'test_image': test_img}\n",
    "    \n",
    "    # Get test descriptor\n",
    "    test_desc = test_descriptors.get(test_img)\n",
    "    \n",
    "    # Extract vertex indices and compute scores\n",
    "    for rank, train_img in enumerate(matched_train_imgs, start=1):\n",
    "        # Parse vertex index: vertex_005_000034_1751340273862754844.png -> vertex_idx = 5\n",
    "        parts = train_img.split('_')\n",
    "        if parts[0] == 'vertex' and len(parts) >= 2:\n",
    "            v_idx = int(parts[1])\n",
    "            row[f'rank_{rank}_vertex'] = v_idx\n",
    "            row[f'rank_{rank}_train_img'] = train_img\n",
    "            \n",
    "            # Compute similarity score\n",
    "            if test_desc is not None and train_img in train_descriptors:\n",
    "                train_desc = train_descriptors[train_img]\n",
    "                score = compute_similarity(test_desc, train_desc)\n",
    "                row[f'rank_{rank}_score'] = float(score)\n",
    "            else:\n",
    "                row[f'rank_{rank}_score'] = None\n",
    "        else:\n",
    "            row[f'rank_{rank}_vertex'] = None\n",
    "            row[f'rank_{rank}_train_img'] = train_img\n",
    "            row[f'rank_{rank}_score'] = None\n",
    "    \n",
    "    results.append(row)\n",
    "\n",
    "# Create DataFrame\n",
    "matches_df = pd.DataFrame(results)\n",
    "\n",
    "# Reorder columns for better readability\n",
    "cols = ['test_image']\n",
    "for i in range(1, 6):  # top 5\n",
    "    cols.append(f'rank_{i}_vertex')\n",
    "    cols.append(f'rank_{i}_score')\n",
    "    cols.append(f'rank_{i}_train_img')\n",
    "matches_df = matches_df[cols]\n",
    "\n",
    "print(f\"\\nCreated DataFrame with {len(matches_df)} test images\")\n",
    "print(f\"\\nFirst 5 rows:\")\n",
    "print(matches_df.head(5))\n",
    "\n",
    "# Save to CSV\n",
    "matches_csv_path = outputs / \"test_to_vertex_matches.csv\"\n",
    "matches_df.to_csv(matches_csv_path, index=False)\n",
    "print(f\"\\n✓ Saved to: {matches_csv_path}\")\n",
    "\n",
    "# Show summary statistics\n",
    "print(f\"\\nSimilarity Score Statistics:\")\n",
    "score_cols = [f'rank_{i}_score' for i in range(1, 6)]\n",
    "for col in score_cols:\n",
    "    print(f\"  {col}: mean={matches_df[col].mean():.4f}, min={matches_df[col].min():.4f}, max={matches_df[col].max():.4f}\")\n",
    "\n",
    "# Display full DataFrame (uncomment to see all)\n",
    "# matches_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wdfbmr8c7th",
   "metadata": {},
   "outputs": [],
   "source": "# Map and vertices paths (SLAM data)\nmap_dir = Path(\"./data/map\")\nvertices_path = map_dir / \"vertices.csv\"\n\n# Load map metadata\nwith open(map_dir / \"map.yaml\", 'r') as f:\n    map_meta = yaml.safe_load(f)\n\nprint(\"Map metadata:\")\nprint(f\"  Resolution: {map_meta['resolution']} m/pixel\")\nprint(f\"  Origin: {map_meta['origin']}\")\n\n# Load map image\nmap_img = plt.imread(map_dir / \"map.pgm\")\nprint(f\"  Map size: {map_img.shape}\")\n\n# Load vertices\nvertices = pd.read_csv(vertices_path)\nprint(f\"\\nLoaded {len(vertices)} vertices\")\n\n# Extract positions\npositions = vertices[[' position x [m]', ' position y [m]']].to_numpy()\n\n# Convert world coordinates (meters) to pixel coordinates\norigin_x, origin_y = map_meta['origin'][0], map_meta['origin'][1]\nresolution = map_meta['resolution']\n\n# Transform: pixel_x = (world_x - origin_x) / resolution\npixel_x = (positions[:, 0] - origin_x) / resolution\npixel_y = (positions[:, 1] - origin_y) / resolution\n\n# Invert Y axis (image origin is top-left, but map origin is bottom-left)\npixel_y = map_img.shape[0] - pixel_y\n\n# Visualize\nfig, ax = plt.subplots(figsize=(14, 12))\n\n# Show map\nax.imshow(map_img, cmap='gray', origin='upper', alpha=0.7)\n\n# Overlay vertices with color gradient (trajectory sequence)\ncolors = np.arange(len(positions))\nscatter = ax.scatter(pixel_x, pixel_y, c=colors, cmap='jet', s=20, alpha=0.8, edgecolors='white', linewidths=0.5)\n\n# Add colorbar to show trajectory direction\ncbar = plt.colorbar(scatter, ax=ax)\ncbar.set_label('Vertex sequence', rotation=270, labelpad=20)\n\n# Mark start and end\nax.scatter(pixel_x[0], pixel_y[0], c='lime', s=150, marker='o', edgecolors='black', linewidths=2, label='Start', zorder=10)\nax.scatter(pixel_x[-1], pixel_y[-1], c='red', s=150, marker='X', edgecolors='black', linewidths=2, label='End', zorder=10)\n\nax.set_title('SLAM Vertices on 2D Occupancy Grid Map', fontsize=14, fontweight='bold')\nax.legend(loc='upper right')\nax.axis('off')\n\n# Save to outputs\nmap_viz_path = outputs / \"all_vertices_on_map.png\"\nplt.savefig(map_viz_path, dpi=150, bbox_inches='tight')\nprint(f\"\\nSaved to: {map_viz_path}\")\n\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "id": "e9pi4ybumbq",
   "metadata": {},
   "source": [
    "### Visualize SLAM Vertices on Map\n",
    "\n",
    "Each reference image is linked to a **SLAM vertex** — a known pose on the map."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22eh290cs",
   "metadata": {},
   "source": [
    "### Visualize Matched Vertices on Map\n",
    "\n",
    "For each test image, show which reference vertices (locations) were retrieved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dnzp4x047t",
   "metadata": {},
   "outputs": [],
   "source": "# Map and vertices paths\nmap_dir = Path(\"./data/map\")\nvertices_path = map_dir / \"vertices.csv\"\n\n# Load map metadata\nwith open(map_dir / \"map.yaml\", 'r') as f:\n    map_meta = yaml.safe_load(f)\n\nprint(\"Map metadata:\")\nprint(f\"  Resolution: {map_meta['resolution']} m/pixel\")\nprint(f\"  Origin: {map_meta['origin']}\")\n\n# Load map image\nmap_img = plt.imread(map_dir / \"map.pgm\")\nprint(f\"  Map size: {map_img.shape}\")\n\n# Load all vertices\nvertices = pd.read_csv(vertices_path)\nprint(f\"\\nLoaded {len(vertices)} vertices\")\n\n# Extract all vertex positions\nall_positions = vertices[[' position x [m]', ' position y [m]']].to_numpy()\n\n# Conversion function\ndef world_to_pixel(world_x, world_y):\n    \"\"\"Convert world coordinates (meters) to pixel coordinates\"\"\"\n    origin_x, origin_y = map_meta['origin'][0], map_meta['origin'][1]\n    resolution = map_meta['resolution']\n    \n    pixel_x = (world_x - origin_x) / resolution\n    pixel_y = (world_y - origin_y) / resolution\n    \n    # Invert Y axis (image origin is top-left, map origin is bottom-left)\n    pixel_y = map_img.shape[0] - pixel_y\n    \n    return pixel_x, pixel_y\n\n# Convert all positions to pixels for background plot\nall_pixel_x, all_pixel_y = world_to_pixel(all_positions[:, 0], all_positions[:, 1])\n\n# Create output directory for map plots\nmap_plots_dir = outputs / \"vertex_match_maps\"\nmap_plots_dir.mkdir(exist_ok=True)\n\nprint(f\"\\nGenerating map visualizations for {len(matches_df)} test images...\")\n\n# Define colors for ranks (rank 1 = best match = green, lower ranks = yellow/orange/red)\nrank_colors = ['lime', 'yellow', 'orange', 'coral', 'red']\nrank_sizes = [150, 120, 100, 80, 60]  # Size decreases with rank\n\n# Iterate through each test image\nfor idx, row in matches_df.iterrows():\n    fig, ax = plt.subplots(figsize=(14, 12))\n    \n    # Show map as background\n    ax.imshow(map_img, cmap='gray', origin='upper', alpha=0.7)\n    \n    # Plot all vertices as background (light gray)\n    ax.scatter(all_pixel_x, all_pixel_y, c='lightgray', s=3, alpha=0.3, label='All vertices')\n    \n    # Extract and plot top 5 matched vertices\n    for rank in range(1, 6):\n        v_idx = row[f'rank_{rank}_vertex']\n        score = row[f'rank_{rank}_score']\n        \n        if pd.notna(v_idx):\n            v_idx = int(v_idx)\n            if v_idx < len(vertices):\n                v_pos_x = vertices.iloc[v_idx][' position x [m]']\n                v_pos_y = vertices.iloc[v_idx][' position y [m]']\n                \n                px, py = world_to_pixel(v_pos_x, v_pos_y)\n                \n                ax.scatter(px, py, \n                          c=rank_colors[rank-1], \n                          s=rank_sizes[rank-1], \n                          marker='o',\n                          edgecolors='black', \n                          linewidths=2,\n                          label=f'Rank {rank} (score: {score:.3f})',\n                          zorder=10-rank,\n                          alpha=0.9)\n                \n                ax.text(px, py, str(rank), \n                       fontsize=10, fontweight='bold',\n                       ha='center', va='center',\n                       color='black', zorder=15)\n    \n    test_img_name = row['test_image']\n    ax.set_title(f'Top-5 Matched Vertices for: {test_img_name}', fontsize=14, fontweight='bold')\n    ax.legend(loc='upper right', fontsize=10, framealpha=0.9)\n    ax.axis('off')\n    \n    save_path = map_plots_dir / f\"vertex_map_test_{idx+1:03d}.png\"\n    plt.savefig(save_path, dpi=150, bbox_inches='tight')\n    plt.close(fig)\n    \n    if (idx + 1) % 10 == 0:\n        print(f\"  Progress: {idx+1}/{len(matches_df)} saved\")\n\nprint(f\"\\nAll {len(matches_df)} map plots saved to: {map_plots_dir}\")"
  },
  {
   "cell_type": "markdown",
   "id": "2jxspqp5ntc",
   "metadata": {},
   "source": [
    "### Initialize Milvus Vector Database\n",
    "\n",
    "We use **Milvus Lite** — a lightweight vector database for fast similarity search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "umee4pqhke8",
   "metadata": {},
   "outputs": [],
   "source": "# Import NetVLADMilvusDB helper\nutils_path = Path(\"./utils\")\nif str(utils_path) not in sys.path:\n    sys.path.insert(0, str(utils_path))\n\nfrom vector_DB_IO import NetVLADMilvusDB\n\nprint(\"NetVLADMilvusDB imported successfully\")"
  },
  {
   "cell_type": "markdown",
   "id": "ozyeyftptgo",
   "metadata": {},
   "source": [
    "### Insert Reference Features (Build the Place Memory)\n",
    "\n",
    "Insert all training image descriptors into Milvus with their vertex IDs (pose references from SLAM)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "zlxq54i2l9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check actual NetVLAD dimension first\n",
    "with h5py.File(feature_path_train, 'r') as f:\n",
    "    sample_img = list(f.keys())[0]\n",
    "    sample_desc = f[sample_img]['global_descriptor'][:]\n",
    "    actual_dim = sample_desc.shape[0]\n",
    "    print(f\"Actual NetVLAD dimension: {actual_dim}\")\n",
    "\n",
    "# Initialize Milvus database\n",
    "milvus_db_path = outputs / \"milvus_netvlad.db\"\n",
    "db_manager = NetVLADMilvusDB(db_path=milvus_db_path, collection_name=\"tornare_train_references\")\n",
    "\n",
    "# Create collection with correct dimension (drop old if exists to start fresh)\n",
    "db_manager.create_collection(dim=actual_dim, drop_old=True)\n",
    "\n",
    "# Insert training features into Milvus\n",
    "print(\"\\n=== Inserting training features into Milvus ===\")\n",
    "num_inserted = db_manager.insert_from_h5(feature_path_train)\n",
    "\n",
    "print(f\"\\n✓ Milvus database ready with {num_inserted} reference images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wq7pjnm6kgi",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 2 — Querying Place Memory (Online Localization)\n",
    "\n",
    "Once the place memory is built, we use it for **online localization**: given the robot's current camera view, find where it might be on the map.\n",
    "\n",
    "## Step 1 — Current View → Global Descriptor\n",
    "\n",
    "At runtime, the robot captures a new image and passes it through the **same NetVLAD pipeline**:\n",
    "1. CNN backbone extracts feature maps\n",
    "2. VLAD layer aggregates them into a global descriptor\n",
    "\n",
    "Using exactly the same model is important — descriptors must live in the **same feature space** as stored memories.\n",
    "\n",
    "## Step 2 — KNN Search in Vector DB\n",
    "\n",
    "The query vector is sent to the vector database for **K-nearest-neighbor search**.\n",
    "\n",
    "The DB returns top-K most similar place memories:\n",
    "```\n",
    "Top-1: \"corridor_03.png\", similarity 0.92\n",
    "Top-2: \"corridor_02.png\", similarity 0.89\n",
    "Top-3: \"lobby_side_01.png\", similarity 0.81\n",
    "```\n",
    "\n",
    "Under the hood, efficient similarity search (L2 distance or cosine similarity) keeps retrieval fast even with thousands of stored descriptors.\n",
    "\n",
    "## Step 3 — Candidates → Coarse Location\n",
    "\n",
    "Each retrieved match includes **pose metadata from SLAM**, so every match delivers a **candidate location on the map**.\n",
    "\n",
    "**VPR's contribution:** Narrow down \"Where am I likely to be?\" to a small set of candidate places.\n",
    "\n",
    "The localization backend (e.g., SfM, PnP) can then verify and compute the final precise pose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "h0yffi6cu9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload the local vpr_core extract_features module to get timing updates\n",
    "import importlib\n",
    "from vpr_core import extract_features\n",
    "importlib.reload(extract_features)\n",
    "\n",
    "print(\"✓ Reloaded extract_features module with timing breakdowns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4x5redaupr",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process one test image: extract vector and search in Milvus DB\n",
    "import time\n",
    "import random\n",
    "\n",
    "total_start = time.time()\n",
    "\n",
    "# Randomly select one test image\n",
    "test_image_path = random.choice(list(images_test.glob(\"*.png\")))\n",
    "print(f\"Processing test image: {test_image_path.name}\")\n",
    "\n",
    "# Step 1: Create temp directory and copy image\n",
    "t1 = time.time()\n",
    "query_dir = outputs / \"single_query\"\n",
    "query_dir.mkdir(exist_ok=True)\n",
    "shutil.copy(test_image_path, query_dir / test_image_path.name)\n",
    "cprint(f\"  [1] File I/O: {time.time() - t1:.3f}s\", 'green')\n",
    "\n",
    "# Step 2: Extract NetVLAD features (convert image to vector)\n",
    "t2 = time.time()\n",
    "query_h5_path = outputs / \"single_query_features.h5\"\n",
    "if query_h5_path.exists():\n",
    "    query_h5_path.unlink()\n",
    "\n",
    "extract_features.main(feature_conf, query_dir, outputs, feature_path=query_h5_path)\n",
    "cprint(f\"  [2] NetVLAD extraction: {time.time() - t2:.3f}s\", 'green')\n",
    "\n",
    "# Step 3: Read the extracted vector\n",
    "t3 = time.time()\n",
    "with h5py.File(query_h5_path, 'r') as f:\n",
    "    img_name = list(f.keys())[0]\n",
    "    query_vector = f[img_name]['global_descriptor'][:]\n",
    "cprint(f\"  [3] Read vector from H5: {time.time() - t3:.3f}s\", 'green')\n",
    "print(f\"      Vector shape: {query_vector.shape}, norm: {np.linalg.norm(query_vector):.2f}\")\n",
    "\n",
    "# Step 4: Search in Milvus database\n",
    "t4 = time.time()\n",
    "matches = db_manager.search(query_vector, top_k=5)\n",
    "cprint(f\"  [4] Milvus search: {time.time() - t4:.3f}s\", 'green')\n",
    "\n",
    "# Step 5: Display results\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"Query: {test_image_path.name}\")\n",
    "print(f\"{'='*70}\")\n",
    "for rank, (match_name, vertex_id, similarity) in enumerate(matches, 1):\n",
    "    print(f\"Rank {rank}: {match_name:50s} (vertex {vertex_id:3d}, sim: {similarity:.4f})\")\n",
    "\n",
    "# Cleanup\n",
    "t5 = time.time()\n",
    "shutil.rmtree(query_dir)\n",
    "query_h5_path.unlink()\n",
    "cprint(f\"\\n  [5] Cleanup: {time.time() - t5:.3f}s\", 'green')\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "cprint(f\"Total time: {time.time() - total_start:.3f}s\", 'yellow')\n",
    "print(f\"{'='*70}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8wuop4hut3p",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare Milvus results with original pairs_from_retrieval results\n",
    "query_img_name = test_image_path.name\n",
    "\n",
    "# Get original results from matches_df\n",
    "original_row = matches_df[matches_df['test_image'] == query_img_name]\n",
    "\n",
    "if len(original_row) > 0:\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Comparison for: {query_img_name}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"{'Rank':<6} {'Original Method':<35} {'Milvus DB':<35}\")\n",
    "    print(f\"{'-'*70}\")\n",
    "    \n",
    "    for rank in range(1, 6):\n",
    "        # Original results\n",
    "        orig_img = original_row.iloc[0][f'rank_{rank}_train_img']\n",
    "        orig_vertex = original_row.iloc[0][f'rank_{rank}_vertex']\n",
    "        orig_score = original_row.iloc[0][f'rank_{rank}_score']\n",
    "        \n",
    "        # Milvus results\n",
    "        milvus_img, milvus_vertex, milvus_score = matches[rank-1]\n",
    "        \n",
    "        # Check if same\n",
    "        same = \"✓\" if orig_img == milvus_img else \"✗\"\n",
    "        \n",
    "        print(f\"{rank:<6} v{orig_vertex} ({orig_score:.3f})  {same:^6}  v{milvus_vertex} ({milvus_score:.3f})\")\n",
    "    \n",
    "    print(f\"{'='*70}\")\n",
    "else:\n",
    "    print(f\"Warning: {query_img_name} not found in original results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gdob73rhziw",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Release GPU memory after VPR\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"GPU cache cleared. Memory released for other tasks.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c933a17",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 3 — (Optional) Multi-Frame Aggregation\n",
    "\n",
    "Even with strong descriptors like NetVLAD, a single frame may not always be reliable — especially in environments with **repetitive structures** (corridors) or **motion blur**.\n",
    "\n",
    "## Strategy: Use Multiple Recent Frames\n",
    "\n",
    "Instead of trusting a single frame, use the **last N frames** (e.g., 3-5 images):\n",
    "1. Perform VPR search for each frame (top-K results)\n",
    "2. Aggregate all candidate matches\n",
    "3. Find consensus among observations\n",
    "\n",
    "## Aggregation Methods\n",
    "\n",
    "**Method A — Frequency Clustering**  \n",
    "Count how many times each candidate location appears across all frames. More consistent results → more likely correct.\n",
    "\n",
    "**Method B — Spatial Voting**  \n",
    "Each candidate has a known pose → group nearby poses on the map. The largest cluster → best location estimate.\n",
    "\n",
    "**Method C — Heatmap Filtering**  \n",
    "Plot match scores onto the map → regions with higher accumulated scores → more confident hypothesis.\n",
    "\n",
    "## Why This Helps\n",
    "\n",
    "Indoor motion is usually **continuous and smooth**. If frame 1 says \"corridor A\" but frames 2-5 say \"lobby area\", the system can confidently discard the outlier.\n",
    "\n",
    "**With only lightweight post-processing, VPR accuracy and robustness improve significantly.**\n",
    "\n",
    "---\n",
    "\n",
    "# Summary\n",
    "\n",
    "In this tutorial, we built a complete VPR-based localization system:\n",
    "\n",
    "1. **NetVLAD** converts camera frames into global descriptors (place signatures)\n",
    "2. **Vector Database** stores these descriptors with pose metadata from SLAM\n",
    "3. **Online Query** retrieves the most similar places for coarse localization\n",
    "4. **(Optional) Multi-frame aggregation** improves reliability\n",
    "\n",
    "**SLAM provides precise motion tracking. VPR provides long-term place recognition.**  \n",
    "Combined, they enable reliable indoor localization over time."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vpr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}